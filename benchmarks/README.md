# Benchmarks

Performance comparison between Rikta, NestJS, and Fastify.

## ğŸš€ Quick Start

```bash
npm install
npm run bench
```

## ğŸ“Š Available Benchmarks

### Startup Time (`npm run bench:startup`)

Measures framework initialization time from module import to server ready.

```bash
npm run bench:startup
```

### Request Overhead (`npm run bench:requests`)

Measures single-request latency with no concurrent load.

```bash
npm run bench:requests
```

### Load Testing (`npm run bench:autocannon`)

High-concurrency throughput testing using Autocannon.

```bash
npm run bench:autocannon
```

## ğŸ¯ Results Summary

| Metric | Rikta vs NestJS | Rikta vs Fastify |
|--------|-----------------|------------------|
| Startup | ğŸŸ¢ **-53% faster** | ğŸŸ¢ **-6% faster** |
| Throughput | ğŸŸ¢ **+7% faster** | ğŸŸ¡ **~5% slower** |
| Latency | ğŸŸ¢ **~35% faster** | ğŸŸ¡ **competitive** |

**Key Takeaway:** Rikta is significantly faster than NestJS and adds minimal overhead over vanilla Fastify.

See [RESULTS.md](./RESULTS.md) for detailed results.

## ğŸ”§ Test Configuration

### Rikta (Optimized)
```typescript
const app = await Rikta.create({
  port: 3001,
  silent: true,   // No console output
  logger: false   // No Fastify logging
});
```

### NestJS
```typescript
const app = await NestFactory.create(
  AppModule,
  new FastifyAdapter({ logger: false })
);
```

### Fastify (Baseline)
```typescript
const app = Fastify({ logger: false });
```

## ğŸ“ Structure

```
benchmarks/
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ fastify-fixture.ts    # Pure Fastify server
â”‚   â”œâ”€â”€ nestjs-fixture.ts     # NestJS server
â”‚   â””â”€â”€ rikta-fixture.ts      # Rikta server
â”œâ”€â”€ startup.bench.ts          # Startup time benchmark
â”œâ”€â”€ request-overhead.bench.ts # Request latency benchmark
â”œâ”€â”€ autocannon.bench.ts       # Load testing
â”œâ”€â”€ RESULTS.md                # Detailed results
â””â”€â”€ QUICK-SUMMARY.md          # Summary table
```

## ğŸ§ª Methodology

### Startup Benchmark
1. Fork child process per framework
2. Measure time from process start to "ready" message
3. Run 5 iterations, take median
4. Fresh process each iteration

### Request Overhead
1. Start all frameworks (different ports)
2. Warm up with 10 requests each
3. Measure 100 sequential requests
4. Calculate median latency

### Load Testing
1. Concurrent connections: 10-100
2. Duration: 10 seconds
3. Measure requests/second and latency percentiles

## ğŸ’¡ Tips

### For Best Results
- Run on Linux for consistent timing
- Close other applications
- Run multiple times, compare medians
- Use `silent: true` and `logger: false`

### Interpreting Results

- **Startup**: Lower is better. Important for serverless/cold starts.
- **Request Latency**: Lower is better. Measures framework overhead.
- **Throughput**: Higher is better. Measures sustained load capacity.

## ğŸ“š Related Documentation

- [Optimization Guide](../docs/OPTIMIZATION.md) - Details on performance optimizations
- [Architecture](../docs/guide/architecture.md) - Framework architecture overview
